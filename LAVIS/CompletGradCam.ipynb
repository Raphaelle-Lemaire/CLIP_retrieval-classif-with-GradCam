{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d451ac0-07ad-4483-b90e-7d625c80257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import gc\n",
    "\n",
    "import urllib.request\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy.ndimage import filters\n",
    "from torch import nn\n",
    "\n",
    "import clip\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import torch.distributed as dist\n",
    "from lavis.common.dist_utils import get_rank, get_world_size, is_main_process, is_dist_avail_and_initialized\n",
    "from lavis.common.logger import MetricLogger, SmoothedValue\n",
    "from lavis.common.registry import registry\n",
    "from lavis.datasets.data_utils import prepare_sample\n",
    "from lavis.models.clip_models.model import CLIP, load_state_dict, load_openai_model, build_model_from_openai_state_dict\n",
    "from lavis.models.clip_models import tokenizer\n",
    "from lavis.processors.clip_processors import ClipImageTrainProcessor\n",
    "from lavis.processors.blip_processors import BlipCaptionProcessor\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from omnixai.data.text import Text\n",
    "from omnixai.data.image import Image\n",
    "from omnixai.data.multi_inputs import MultiInputs\n",
    "from omnixai.preprocessing.image import Resize\n",
    "from omnixai.explainers.vision_language.specific.gradcam import GradCAM\n",
    "\n",
    "from lavis.models.clip_models.pretrained import (\n",
    "    download_pretrained,\n",
    "    get_pretrained_url,\n",
    "    list_pretrained_tag_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7604615-4573-4902-9507-ecfbb56b08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Fmodel_clip(chekpoint=True, vit=True):\n",
    "    preprocess=None\n",
    "    if vit:\n",
    "        if chekpoint:\n",
    "            model_cfg ={\n",
    "            \"embed_dim\": 768,\n",
    "            \"quick_gelu\": True,\n",
    "            \"vision_cfg\": {\n",
    "                \"image_size\": 336,\n",
    "                \"layers\": 24,\n",
    "                \"width\": 1024,\n",
    "                \"patch_size\": 14},\n",
    "            \"text_cfg\": {\n",
    "                \"context_length\": 77,\n",
    "                \"vocab_size\": 49408,\n",
    "                \"width\": 768,\n",
    "                \"heads\": 12,\n",
    "                \"layers\": 12}}\n",
    "            model = CLIP(**model_cfg, add_cls_token=False)\n",
    "            checkpoint_path = \"lavis/checkpoint_best (1).pth\"\n",
    "            model.load_state_dict(load_state_dict(checkpoint_path))\n",
    "        else:\n",
    "            model_cfg ={\n",
    "            \"embed_dim\": 768,\n",
    "            \"quick_gelu\": True,\n",
    "            \"vision_cfg\": {\n",
    "                \"image_size\": 224,\n",
    "                \"layers\": 24,\n",
    "                \"width\": 1024,\n",
    "                \"patch_size\": 14},\n",
    "            \"text_cfg\": {\n",
    "                \"context_length\": 77,\n",
    "                \"vocab_size\": 49408,\n",
    "                \"width\": 768,\n",
    "                \"heads\": 12,\n",
    "                \"layers\": 12}}\n",
    "            model = CLIP(**model_cfg, add_cls_token=False)\n",
    "            model = load_openai_model(name=\"ViT-L-14\", device=\"cpu\", jit=False)\n",
    "            model = model.float()\n",
    "        _, preprocess = clip.load(\"ViT-L/14\")\n",
    "    else:\n",
    "        model, preprocess = clip.load(\"RN50\", device=\"cuda\")\n",
    "    return model, preprocess\n",
    "\n",
    "m,p=load_Fmodel_clip(chekpoint=False, vit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb44e7-dbc9-4426-a521-3793b1bb4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCamViT:\n",
    "    def __init__(self, model, target, height=24):\n",
    "        self.model = model.eval()  \n",
    "        self.handlers = [] \n",
    "        self.target = target\n",
    "        self.height=height\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.model.token_embedding(text)\n",
    "        x = x + self.model.positional_embedding\n",
    "        \n",
    "        x = x.permute(1, 0, 2) \n",
    "        x = self.model.transformer(x, attn_mask=self.model.attn_mask)\n",
    "        x = x.permute(1, 0, 2) \n",
    "        \n",
    "        x = self.model.ln_final(x)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.model.text_projection\n",
    "        return x\n",
    "\n",
    "    def image_attn_blocks(self, size):\n",
    "        image_attn_blocks = list(dict(self.model.visual.transformer.resblocks.named_children()).values())\n",
    "        num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
    "            \n",
    "        cam = image_attn_blocks[-1].attn_grad.detach()\n",
    "        cam = cam.mean(dim=0) \n",
    "        cam = cam[0, 1:]\n",
    "        cam = cam.reshape(1, 1, size, size)\n",
    "        cam = torch.nn.functional.interpolate(cam, size=224, mode='bilinear')\n",
    "        cam = cam.reshape(224, 224)\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        return cam\n",
    "\n",
    "    def txt_attn_blocks(self, index):\n",
    "        txt_attn_blocks = list(dict(self.model.transformer.resblocks.named_children()).values())\n",
    "        num_tokens = txt_attn_blocks[0].attn_grad.shape[-1]\n",
    "        cam = txt_attn_blocks[-1].attn_grad.detach()\n",
    "        cam = cam.mean(dim=0)\n",
    "        cam = cam[index]\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        return cam\n",
    "            \n",
    "\n",
    "    def __call__(self, inputs, val, index, size):\n",
    "        self.model.zero_grad()  # Zero the gradients\n",
    "\n",
    "        image=inputs[\"im\"]\n",
    "        text=inputs[\"txt\"]\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        image_features = self.model.visual(image)\n",
    "        image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
    "        image_features = image_features / image_features_norm\n",
    "        \n",
    "        target_features = self.encode_text(text)\n",
    "        target_features_norm = target_features.norm(dim=-1, keepdim=True)\n",
    "        target_features = target_features / target_features_norm\n",
    "        \n",
    "        similarity = image_features[0].dot(target_features[0])\n",
    "        self.model.zero_grad()\n",
    "        similarity.backward(retain_graph=True)\n",
    "\n",
    "        cam_img = self.image_attn_blocks(size)\n",
    "        cam_txt = self.txt_attn_blocks(index)\n",
    "        \n",
    "        return cam_img.cpu().data.numpy(), cam_txt.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497273a-a773-4243-9f9b-db09a05e3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_text_xticks(sentence):\n",
    "    tokens = [word_.strip() for word_ in sentence.split('<\\w>')][:77]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131137eb-6c87-4bc5-8381-d9310e0be163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_score(vec, pred_text, xticks, baton=True):\n",
    "    xticks=xticks[:xticks.index('<end_of_text>')+1]\n",
    "    vec=vec[:len(xticks)]\n",
    "    _axis_fontsize=13\n",
    "    if baton==True:\n",
    "        fig=plt.figure(figsize = (len(vec)+1,10))\n",
    "        \n",
    "        colors = plt.cm.viridis(vec)\n",
    "        \n",
    "        plt.bar(range(len(vec)), vec, color=colors, edgecolor='black', align='center')\n",
    "        plt.yticks([0, 0.5, 1], fontsize=_axis_fontsize)\n",
    "        plt.xticks(range(0,len(vec)+1), xticks+[\" \"], fontsize=_axis_fontsize)\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=0, vmax=1))\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, orientation='horizontal', pad=0.2)\n",
    "        cbar.set_label('Value Intensity', fontsize=12)\n",
    "        \n",
    "        plt.figtext(x=0.13, y=0.54, s='Prediction: {}'.format(pred_text), fontsize=15, fontname='sans-serif')\n",
    "    else:\n",
    "        fig=plt.figure(figsize = (len(vec)+1,2))\n",
    "        \n",
    "        plt.yticks([])\n",
    "        plt.xticks(range(0,len(vec)+1), xticks+[\" \"], fontsize=_axis_fontsize)\n",
    "        fig.add_subplot(1, 1, 1)\n",
    "        plt.figtext(x=0.13, y=0.54, s='Prediction: {}'.format(pred_text), fontsize=15, fontname='sans-serif')\n",
    "        \n",
    "        img = plt.imshow([vec], vmin=0, vmax=1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de222b8-2bca-455c-93fa-9d453db9a7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b211af0-d378-498d-adca-3a8aa119812a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba3e5e-5e1c-4b3f-a26b-4d4b0a02b461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b57df-8c51-4de6-93ed-71919462dd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b289f-9c10-4ffe-802d-435e163cfebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806b715-5f60-4891-ad27-dc34c589c349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
