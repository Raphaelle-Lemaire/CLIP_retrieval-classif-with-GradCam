{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf09e74-c056-4d2c-aab2-91ac8214e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ligne 1: 10.0: Aucun fichier ou dossier de ce nom\n"
     ]
    }
   ],
   "source": [
    "!pip install omnixai pillow<10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f5be15-d4e2-4f91-9788-80b82cb0361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import gc\n",
    "\n",
    "import urllib.request\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy.ndimage import filters\n",
    "from torch import nn\n",
    "\n",
    "import clip\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import torch.distributed as dist\n",
    "from lavis.common.dist_utils import get_rank, get_world_size, is_main_process, is_dist_avail_and_initialized\n",
    "from lavis.common.logger import MetricLogger, SmoothedValue\n",
    "from lavis.common.registry import registry\n",
    "from lavis.datasets.data_utils import prepare_sample\n",
    "from lavis.models.clip_models.model import CLIP, load_state_dict, load_openai_model\n",
    "from lavis.processors.clip_processors import ClipImageTrainProcessor\n",
    "from lavis.processors.blip_processors import BlipCaptionProcessor\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from omnixai.data.text import Text\n",
    "from omnixai.data.image import Image\n",
    "from omnixai.data.multi_inputs import MultiInputs\n",
    "from omnixai.preprocessing.image import Resize\n",
    "from omnixai.explainers.vision_language.specific.gradcam import GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fffe06-3a57-47dc-9840-e80b782c9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Fmodel_clip(chekpoint=True, vit=True):\n",
    "    preprocess=None\n",
    "    if vit:\n",
    "        model_cfg ={\n",
    "            \"embed_dim\": 768,\n",
    "            \"quick_gelu\": True,\n",
    "            \"vision_cfg\": {\n",
    "                \"image_size\": 336,\n",
    "                \"layers\": 24,\n",
    "                \"width\": 1024,\n",
    "                \"patch_size\": 14\n",
    "            },\n",
    "            \"text_cfg\": {\n",
    "                \"context_length\": 77,\n",
    "                \"vocab_size\": 49408,\n",
    "                \"width\": 768,\n",
    "                \"heads\": 12,\n",
    "                \"layers\": 12\n",
    "            }\n",
    "        }\n",
    "        model = CLIP(**model_cfg, add_cls_token=False)\n",
    "        if chekpoint:\n",
    "            checkpoint_path = \"lavis/checkpoint_best (1).pth\"\n",
    "            model.load_state_dict(load_state_dict(checkpoint_path))\n",
    "        _, preprocess = clip.load(\"ViT-L/14\")\n",
    "    else:\n",
    "        model, preprocess = clip.load(\"RN50\", device=\"cuda\")\n",
    "    del _\n",
    "    return model, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7dbc83e-4a74-481f-a915-4f787cbe6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model,dico):\n",
    "    im,txt = dico['im'],dico['txt']\n",
    "    txt_enc = BlipCaptionProcessor(max_words=77)(txt)\n",
    "    txt_enc= model.tokenizer(txt_enc, add_cls_token=True)\n",
    "    txt_enc,txt_enc_class=model.encode_text(txt_enc.to(dtype=torch.int64))\n",
    "    txt_enc=F.normalize(txt_enc, dim=-1)\n",
    "\n",
    "    image=ClipImageTrainProcessor(image_size=336)(im)\n",
    "    feat_im = model.encode_image(image[None])\n",
    "    feat_im =F.normalize(feat_im, dim=-1)\n",
    "    del im,txt, txt_enc_class, image\n",
    "    return txt_enc @ feat_im.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75963a56-45a5-480e-89e5-92b7bb882537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x: np.ndarray) -> np.ndarray:\n",
    "    # Normalize to [0, 1].\n",
    "    x = x - x.min()\n",
    "    if x.max() > 0:\n",
    "        x = x / x.max()\n",
    "    return x\n",
    "\n",
    "# Modified from: https://github.com/salesforce/ALBEF/blob/main/visualization.ipynb\n",
    "def getAttMap(img, attn_map, blur=True):\n",
    "    if blur:\n",
    "        attn_map = filters.gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
    "    attn_map = normalize(attn_map)\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
    "    attn_map = 1*(1-attn_map**0.7).reshape(attn_map.shape + (1,))*img + \\\n",
    "            (attn_map**0.7).reshape(attn_map.shape+(1,)) * attn_map_c\n",
    "    del attn_map_c, cmap, blur\n",
    "    return attn_map\n",
    "\n",
    "def viz_attn(img, attn_map, blur=True, title=\"titre\"):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[1].imshow(getAttMap(img, attn_map, blur))\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "    del fig,axes, img, attn_map, blur, title\n",
    "    \n",
    "def load_image(img_path, resize=None):\n",
    "    image = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "    if resize is not None:\n",
    "        image = image.resize((resize, resize))\n",
    "    del resize\n",
    "    return np.asarray(image).astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318e6086-2ea7-4940-8eef-ccdec9b45f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.data = None\n",
    "        self.hook = module.register_forward_hook(self.save_grad)\n",
    "        \n",
    "    def save_grad(self, module, input, output):\n",
    "        self.data = output\n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.hook.remove()\n",
    "        \n",
    "    @property\n",
    "    def activation(self) -> torch.Tensor:\n",
    "        return self.data\n",
    "    \n",
    "    @property\n",
    "    def gradient(self) -> torch.Tensor:\n",
    "        return self.data.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b79612-8a65-401b-833a-e40c2de48fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCamViT:\n",
    "    def __init__(self, model, target, height=24):\n",
    "        self.model = model.eval()\n",
    "        self.feature = None \n",
    "        self.gradient = None r\n",
    "        self.handlers = [] \n",
    "        self.target = target\n",
    "        self._get_hook() \n",
    "        self.height=height\n",
    "\n",
    "    def _get_features_hook(self, module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output=output[0]\n",
    "        self.feature = self.reshape_transform(output)\n",
    "\n",
    "    def _get_grads_hook(self, module, input_grad, output_grad):\n",
    "        if isinstance(output_grad, tuple):\n",
    "            output_grad=output_grad[0]\n",
    "        self.gradient = self.reshape_transform(output_grad)\n",
    "\n",
    "        def _store_grad(grad):\n",
    "            self.gradient = self.reshape_transform(grad) \n",
    "\n",
    "\n",
    "    def _get_hook(self):\n",
    "        self.target.register_forward_hook(self._get_features_hook)\n",
    "        self.target.register_forward_hook(self._get_grads_hook)\n",
    "\n",
    "    def reshape_transform(self, tensor):\n",
    "        if tensor.shape[1]==1:\n",
    "            tensor = tensor.permute(1, 0, 2)\n",
    "        result = tensor[:, 1:, :].reshape(tensor.size(0), self.height, self.height, tensor.size(2))\n",
    "        result = result.transpose(2, 3).transpose(1, 2) \n",
    "        return result\n",
    "\n",
    "    def __call__(self, inputs, val):\n",
    "        self.model.zero_grad()  \n",
    "        if True:\n",
    "            image=inputs[\"im\"]\n",
    "            text=inputs[\"txt\"]\n",
    "            target_features=inputs[\"txtF\"]\n",
    "            self.model.visual.eval()\n",
    "            image_features = self.model.visual(image)\n",
    "            image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
    "            image_features_new = image_features / image_features_norm\n",
    "            target_features_norm = target_features.norm(dim=-1, keepdim=True)\n",
    "            target_features_new = target_features / target_features_norm\n",
    "            similarity = image_features_new[0].dot(target_features_new[0])\n",
    "            self.model.visual.zero_grad()\n",
    "            similarity.backward(retain_graph=True)\n",
    "            \n",
    "            image_attn_blocks = list(dict(self.model.visual.transformer.resblocks.named_children()).values())\n",
    "            num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
    "            \n",
    "            \n",
    "            cam = image_attn_blocks[-1].attn_grad.detach()\n",
    "            \n",
    "            print(cam.shape)\n",
    "            cam = cam.mean(dim=0) \n",
    "            print(cam.shape)\n",
    "            image_relevance = cam[0, 1:]\n",
    "            print(image_relevance.shape)\n",
    "            image_relevance = image_relevance.reshape(1, 1, 24, 24)\n",
    "            print(image_relevance.shape)\n",
    "            image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "            print(image_relevance.shape)\n",
    "            image_relevance = image_relevance.reshape(224, 224)\n",
    "            print(image_relevance.shape)\n",
    "            image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "            print(image_relevance.shape)\n",
    "            cam=image_relevance\n",
    "            #image=cv2.resize(image, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "            #cam = image_relevance * image\n",
    "            #cam = cam / torch.max(cam)\n",
    "            #cam = cam[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "            #cam = np.float32(cam)\n",
    "            print(cam.max(), cam.min(), cam)\n",
    "            return cam.cpu().data.numpy()\n",
    "\n",
    "\n",
    "        #textes=inputs[\"txt\"]\n",
    "        #produit=[]\n",
    "        #for t in textes:\n",
    "        #    inputs[\"txt\"]=t\n",
    "        #    produit.append(forward(self.model,inputs))\n",
    "        #print(\"dfghj\")\n",
    "        #prod=torch.tensor(produit)\n",
    "        #print(\"dfghj\")\n",
    "        #target=F.softmax(prod, dim=0)\n",
    "        #print(target.shape)\n",
    "        #output = forward(inputs) \n",
    "\n",
    "        #index = np.argmax(output.cpu().data.numpy())\n",
    "        #target = output[0][index]  # Get the target score\n",
    "\n",
    "        gradient = self.gradient[0].cpu().data.numpy()\n",
    "        print(gradient.shape)\n",
    "        weight = np.mean(gradient, axis=(1, 2))  \n",
    "        print(weight.shape)\n",
    "        feature = self.feature[0].cpu().data.numpy()\n",
    "        print(feature.shape)\n",
    "        \n",
    "        cam = feature * weight[:, np.newaxis, np.newaxis]\n",
    "        print(cam.shape)\n",
    "        cam = np.sum(cam, axis=0) \n",
    "        print(cam.shape)\n",
    "        cam = np.maximum(cam, 0) \n",
    "        print(cam.shape)\n",
    "\n",
    "        cam -= np.min(cam)\n",
    "        print(cam.shape)\n",
    "        cam /= np.max(cam)\n",
    "        print(cam.shape)\n",
    "        return cam  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b6d415-7d17-4946-9691-6d88bb782bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCAMResnet(\n",
    "    model: nn.Module,\n",
    "    input: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    layer: nn.Module\n",
    ") -> torch.Tensor:\n",
    "    if input.grad is not None:\n",
    "        input.grad.data.zero_()\n",
    "        \n",
    "    requires_grad = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        requires_grad[name] = param.requires_grad\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "    assert isinstance(layer, nn.Module)\n",
    "    with Hook(layer) as hook:        \n",
    "        output = model(input)\n",
    "        output.backward(target)\n",
    "\n",
    "        grad = hook.gradient.float()\n",
    "        act = hook.activation.float()\n",
    "    \n",
    "        alpha = grad.mean(dim=(2, 3), keepdim=True)\n",
    "        gradcam = torch.sum(act * alpha, dim=1, keepdim=True)\n",
    "        gradcam = torch.clamp(gradcam, min=0)\n",
    "\n",
    "    gradcam = F.interpolate(\n",
    "        gradcam,\n",
    "        input.shape[2:],\n",
    "        mode='bicubic',\n",
    "        align_corners=False)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad_(requires_grad[name])\n",
    "        \n",
    "    return gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6689ffa9-bb1e-4f9f-830c-645c1bcedcb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 577, 577])\n",
      "torch.Size([577, 577])\n",
      "torch.Size([576])\n",
      "torch.Size([1, 1, 24, 24])\n",
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([224, 224])\n",
      "torch.Size([224, 224])\n",
      "tensor(1.) tensor(0.) tensor([[0.2857, 0.2857, 0.2857,  ..., 0.2718, 0.2718, 0.2718],\n",
      "        [0.2857, 0.2857, 0.2857,  ..., 0.2718, 0.2718, 0.2718],\n",
      "        [0.2857, 0.2857, 0.2857,  ..., 0.2718, 0.2718, 0.2718],\n",
      "        ...,\n",
      "        [0.2859, 0.2859, 0.2859,  ..., 0.3048, 0.3048, 0.3048],\n",
      "        [0.2859, 0.2859, 0.2859,  ..., 0.3048, 0.3048, 0.3048],\n",
      "        [0.2859, 0.2859, 0.2859,  ..., 0.3048, 0.3048, 0.3048]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52386/681062032.py:11: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  attn_map = filters.gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 577, 577])\n",
      "torch.Size([577, 577])\n",
      "torch.Size([576])\n",
      "torch.Size([1, 1, 24, 24])\n",
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([224, 224])\n",
      "torch.Size([224, 224])\n",
      "tensor(1.) tensor(0.) tensor([[0.3842, 0.3842, 0.3842,  ..., 0.4225, 0.4225, 0.4225],\n",
      "        [0.3842, 0.3842, 0.3842,  ..., 0.4225, 0.4225, 0.4225],\n",
      "        [0.3842, 0.3842, 0.3842,  ..., 0.4225, 0.4225, 0.4225],\n",
      "        ...,\n",
      "        [0.4340, 0.4340, 0.4340,  ..., 0.4480, 0.4480, 0.4480],\n",
      "        [0.4340, 0.4340, 0.4340,  ..., 0.4480, 0.4480, 0.4480],\n",
      "        [0.4340, 0.4340, 0.4340,  ..., 0.4480, 0.4480, 0.4480]])\n",
      "torch.Size([16, 577, 577])\n",
      "torch.Size([577, 577])\n",
      "torch.Size([576])\n",
      "torch.Size([1, 1, 24, 24])\n",
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([224, 224])\n",
      "torch.Size([224, 224])\n",
      "tensor(1.) tensor(0.) tensor([[0.6799, 0.6799, 0.6799,  ..., 0.6754, 0.6754, 0.6754],\n",
      "        [0.6799, 0.6799, 0.6799,  ..., 0.6754, 0.6754, 0.6754],\n",
      "        [0.6799, 0.6799, 0.6799,  ..., 0.6754, 0.6754, 0.6754],\n",
      "        ...,\n",
      "        [0.6866, 0.6866, 0.6866,  ..., 0.6770, 0.6770, 0.6770],\n",
      "        [0.6866, 0.6866, 0.6866,  ..., 0.6770, 0.6770, 0.6770],\n",
      "        [0.6866, 0.6866, 0.6866,  ..., 0.6770, 0.6770, 0.6770]])\n"
     ]
    }
   ],
   "source": [
    "def camGradif(cam, im):\n",
    "    im=ClipImageTrainProcessor(image_size=336)(im)\n",
    "    \n",
    "    cam_resized = cv2.resize(cam, (336,336), interpolation=cv2.INTER_LINEAR)  # Resize to match the input image size\n",
    "\n",
    "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(im.permute(1,2,0).cpu())\n",
    "    axes[1].imshow(-cam_resized)\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    #im2=np.array(im.permute(1, 2, 0).cpu())\n",
    "    #plt.imshow(im2,cmap='gray')\n",
    "    #plt.show()\n",
    "    \n",
    "    #combined_image = cv2.addWeighted(im2.astype(np.float32), 0.5, cam_colored.astype(np.float32), 0.5, 0)\n",
    "    #plt.imshow(combined_image,cmap='gray')\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def pre_forward_resnet(model,dico, preprocess):\n",
    "    blur = True #@param {type:\"boolean\"}\n",
    "    image_input = preprocess(dico[\"im\"]).unsqueeze(0)\n",
    "    text_input = clip.tokenize([dico[\"txt\"]])\n",
    "    text_input=model.encode_text(text_input).float()\n",
    "    return image_input, text_input\n",
    "\n",
    "def afficheDiff(cam1,cam2, im):\n",
    "    cam_différence = (cam1 - cam2)*255\n",
    "\n",
    "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(im)\n",
    "    axes[1].imshow(getAttMap(im, cam_différence, True))\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def lancerCode(vit, chekpoint, im, txt, image_path, title):\n",
    "    model,preprocess=load_Fmodel_clip(chekpoint=chekpoint, vit=vit)\n",
    "    #if vit and chekpoint:\n",
    "    #    text = clip.tokenize(txt)\n",
    "    #    text_feat,_ = model.encode_text(text)\n",
    "    #    cam=GradCamViT(model, model.visual.transformer.resblocks[-1])({\"im\":im,\"txt\":text, \"txtF\":text_feat}, chekpoint)\n",
    "    #    image_np=load_image(image_path, 224)\n",
    "    #    cam = cv2.resize(cam, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "    #    viz_attn(image_np, cam, True, title)\n",
    "    if vit:\n",
    "        image = ClipImageTrainProcessor(image_size=336)(im).unsqueeze(0)\n",
    "        model.add_cls_token=False\n",
    "        text = clip.tokenize(txt)\n",
    "        text_feat,_ = model.encode_text(text)\n",
    "        \n",
    "        cam=GradCamViT(model, model.visual.transformer.resblocks[-1], height=24)({\"im\":image,\"txt\":text, \"txtF\":text_feat}, chekpoint)\n",
    "        #inputs={\"im\":image,\"txt\":text}\n",
    "        #inputs = preprocess(text=[txt], images=[im], return_tensors=\"pt\", padding=True)\n",
    "        #cam=GradCamViT(model, model.visual.transformer.resblocks[-1], height=16)(inputs, chekpoint)\n",
    "        image_np=load_image(image_path, 224)\n",
    "        cam = cv2.resize(cam, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "        viz_attn(image_np, cam, True, title)\n",
    "    else:\n",
    "        image, text=pre_forward_resnet(model,{\"im\":im,\"txt\":txt}, preprocess)\n",
    "        cam=gradCAMResnet(model.visual,image,text,getattr(model.visual, \"layer4\"))\n",
    "        cam = cam.squeeze().detach().cpu().numpy()\n",
    "        image_np=load_image(image_path, model.visual.input_resolution)\n",
    "        viz_attn(image_np, cam, True, title)\n",
    "    return cam, image_np\n",
    "\n",
    "image_path=\"image/imageFC.png\"#\"image.jpg\"\n",
    "image=PIL.Image.open(image_path)\n",
    "\n",
    "im = image#\"They are framed by columns, and are looking towards the detailed city and landscape in the background.\"#,\"The Descent from the Cross (or Deposition of Christ, or Descent of Christ from the Cross) is a panel painting by the Flemish artist Rogier van der Weyden created c. 1435, now in the Museo del Prado, Madrid.\"]\n",
    "\n",
    "#cam1, image = lancerCode(True, True, im, \"Subleyras’s female portraits are recognizable by the round, youthful faces of the models, a characteristic attributed here to Countess Mahony.\", image_path)\n",
    "#cam2, image = lancerCode(True, False, im, \"Subleyras’s female portraits are recognizable by the round, youthful faces of the models, a characteristic attributed here to Countess Mahony.\", image_path)\n",
    "#afficheDiff(cam1,cam2, image)\n",
    "\n",
    "phrase = \"A great lady in negligee is intimate and the velvet drapery recall the social status of the character.\"\n",
    "\n",
    "# Diviser la phrase en mots\n",
    "mots = phrase.split()\n",
    "\n",
    "# Résultats pour chaque mot\n",
    "vit = True\n",
    "chekpoint= False\n",
    "cam_phrase, image_phrase = lancerCode(vit, chekpoint, im, phrase, image_path, phrase)\n",
    "    cam_phrase, image_phrase = lancerCode(vit, chekpoint, im, phrase, image_path, phrase)\n",
    "    cam_phrase, image_phrase = lancerCode(vit, chekpoint, im, phrase, image_path, phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43b55d-b830-4c54-97c7-d2fc6ea29ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232363cc-2aff-4c93-830e-d46b63985106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
